"""
# Anthropic's Original Performance Engineering Take-home (Release version)

Copyright Anthropic PBC 2026. Permission is granted to modify and use, but not
to publish or redistribute your solutions so it's hard to find spoilers.

# Task

- Optimize the kernel (in KernelBuilder.build_kernel) as much as possible in the
  available time, as measured by test_kernel_cycles on a frozen separate copy
  of the simulator.

We recommend you look through problem.py next.
"""

from collections import defaultdict
import os
import random
import unittest

from problem import (
    Engine,
    DebugInfo,
    SLOT_LIMITS,
    VLEN,
    N_CORES,
    SCRATCH_SIZE,
    Machine,
    Tree,
    Input,
    HASH_STAGES,
    reference_kernel,
    build_mem_image,
    reference_kernel2,
)


class KernelBuilder:
    def __init__(self):
        self.instrs = []
        self.scratch = {}
        self.scratch_debug = {}
        self.scratch_ptr = 0
        self.const_map = {}
        self._bundler_limits = SLOT_LIMITS

    def debug_info(self):
        return DebugInfo(scratch_map=self.scratch_debug)

    def build(self, slots: list[tuple[Engine, tuple]], vliw: bool = False):
        # Simple slot packing that just uses one slot per instruction bundle
        instrs = []
        for engine, slot in slots:
            instrs.append({engine: [slot]})
        return instrs

    def add(self, engine, slot):
        self.instrs.append({engine: [slot]})

    def add_bundle(self, bundles: dict[str, list[tuple]]):
        """
        Append a single VLIW instruction bundle.
        `bundles` maps engine -> list[slot]. Empty lists are ignored.
        """
        instr = {k: v for k, v in bundles.items() if v}
        if instr:
            # Basic sanity check
            for eng, slots in instr.items():
                if eng == "debug":
                    continue
                assert (
                    len(slots) <= self._bundler_limits[eng]
                ), f"Too many {eng} slots in bundle"
        self.instrs.append(instr)

    def alloc_scratch(self, name=None, length=1):
        addr = self.scratch_ptr
        if name is not None:
            self.scratch[name] = addr
            self.scratch_debug[addr] = (name, length)
        self.scratch_ptr += length
        assert self.scratch_ptr <= SCRATCH_SIZE, "Out of scratch space"
        return addr

    def scratch_const(self, val, name=None):
        if val not in self.const_map:
            addr = self.alloc_scratch(name)
            self.add("load", ("const", addr, val))
            self.const_map[val] = addr
        return self.const_map[val]

    def _broadcast_const_vec(self, scalar_addr: int, name: str):
        vaddr = self.alloc_scratch(name, VLEN)
        self.add_bundle({"valu": [("vbroadcast", vaddr, scalar_addr)]})
        return vaddr

    def _schedule_round(self, groups_ops: list[list[dict[str, list[tuple]]]]):
        """
        Greedy VLIW scheduler for one round.
        `groups_ops[g]` is a list of per-cycle bundles (each bundle dict engine -> slots),
        with a strict in-order dependency within each group.
        """
        n_groups = len(groups_ops)

        # Fast path for this take-home: our main kernel groups are almost entirely "valu" and "load".
        engines_used: set[str] = set()
        for g in range(n_groups):
            for b in groups_ops[g]:
                engines_used.update(b.keys())
        engines_used.discard("debug")

        if engines_used.issubset({"valu", "load"}):
            pos = [0] * n_groups
            out_instrs: list[dict[str, list[tuple]]] = []

            while True:
                if all(pos[g] >= len(groups_ops[g]) for g in range(n_groups)):
                    self.instrs.extend(out_instrs)
                    return

                cap_valu = self._bundler_limits["valu"]
                cap_load = self._bundler_limits["load"]
                out: dict[str, list[tuple]] = {"valu": [], "load": []}
                used_groups: set[int] = set()

                # Take at most one load bundle (our load bundles are almost always exactly 2 slots).
                load_candidates: list[int] = []
                for g in range(n_groups):
                    pg = pos[g]
                    if pg >= len(groups_ops[g]):
                        continue
                    b = groups_ops[g][pg]
                    slots = b.get("load")
                    if not slots:
                        continue
                    if len(slots) <= cap_load:
                        load_candidates.append(g)

                if load_candidates and cap_load > 0:
                    # Finish load runs quickly to unblock dependent VALU work.
                    g_load = max(load_candidates, key=lambda g: pos[g])
                    b = groups_ops[g_load][pos[g_load]]
                    out["load"].extend(b["load"])
                    cap_load -= len(b["load"])
                    pos[g_load] += 1
                    used_groups.add(g_load)

                # Choose a subset of VALU bundles that best fills the 6-slot budget.
                valu_items: list[tuple[int, int]] = []
                for g in range(n_groups):
                    if g in used_groups:
                        continue
                    pg = pos[g]
                    if pg >= len(groups_ops[g]):
                        continue
                    b = groups_ops[g][pg]
                    slots = b.get("valu")
                    if not slots:
                        continue
                    w = len(slots)
                    if w <= cap_valu:
                        valu_items.append((g, w))

                # 0/1 knapsack with tiny capacity (<=6), maximizing filled slots.
                best_w = [-1] * (cap_valu + 1)
                best_possum = [10**18] * (cap_valu + 1)
                best_pick: list[tuple[int, ...]] = [tuple() for _ in range(cap_valu + 1)]
                best_w[0] = 0
                best_possum[0] = 0
                for g, w in valu_items:
                    for c in range(cap_valu, w - 1, -1):
                        if best_w[c - w] < 0:
                            continue
                        cand_w = best_w[c - w] + w
                        cand_pos = best_possum[c - w] + pos[g]
                        if cand_w > best_w[c] or (cand_w == best_w[c] and cand_pos < best_possum[c]):
                            best_w[c] = cand_w
                            best_possum[c] = cand_pos
                            best_pick[c] = best_pick[c - w] + (g,)

                # Pick the best-filled capacity.
                best_c = max(
                    range(cap_valu + 1),
                    key=lambda c: (best_w[c], -best_possum[c]),
                )
                for g in best_pick[best_c]:
                    b = groups_ops[g][pos[g]]
                    out["valu"].extend(b["valu"])
                    pos[g] += 1
                    used_groups.add(g)

                instr = {k: v for k, v in out.items() if v}
                if not instr:
                    # Shouldn't happen, but ensure progress.
                    for g in range(n_groups):
                        if pos[g] < len(groups_ops[g]):
                            b = groups_ops[g][pos[g]]
                            instr = {k: v for k, v in b.items() if v}
                            pos[g] += 1
                            break
                if instr:
                    out_instrs.append(instr)
            # Unreachable

        def build_schedule(
            *,
            engine_order: tuple[str, ...],
            mem_ahead_compute_behind: bool,
            rng_seed: int | None,
        ) -> list[dict[str, list[tuple]]]:
            import random as _random

            rng = _random.Random(rng_seed) if rng_seed is not None else None
            pos = [0] * n_groups
            out_instrs: list[dict[str, list[tuple]]] = []

            def emit_bundle(bundles: dict[str, list[tuple]]):
                instr = {k: v for k, v in bundles.items() if v}
                if instr:
                    out_instrs.append(instr)

            while True:
                if all(pos[g] >= len(groups_ops[g]) for g in range(n_groups)):
                    return out_instrs

                cap = dict(self._bundler_limits)
                cap.pop("debug", None)
                out: dict[str, list[tuple]] = {k: [] for k in cap.keys()}
                used_groups: set[int] = set()

                def choose_best(engine: str) -> tuple[int, dict[str, list[tuple]]] | None:
                    if cap[engine] <= 0:
                        return None
                    best: list[tuple[tuple, int, dict[str, list[tuple]]]] = []
                    for g in range(n_groups):
                        if g in used_groups:
                            continue
                        pg = pos[g]
                        if pg >= len(groups_ops[g]):
                            continue
                        b = groups_ops[g][pg]
                        slots = b.get(engine)
                        if not slots:
                            continue
                        if len(slots) > cap[engine]:
                            continue
                        ok = True
                        total_slots = 0
                        for eng2, slots2 in b.items():
                            if eng2 == "debug":
                                continue
                            if len(slots2) > cap.get(eng2, 0):
                                ok = False
                                break
                            total_slots += len(slots2)
                        if not ok:
                            continue

                        if mem_ahead_compute_behind:
                            skew = pg if engine in ("load", "store") else -pg
                        else:
                            skew = pg
                        score = (skew, total_slots)
                        best.append((score, g, b))
                    if not best:
                        return None
                    best.sort(reverse=True, key=lambda x: x[0])
                    # Randomize ties slightly to explore different interleavings.
                    if rng is not None and len(best) > 1 and best[0][0] == best[1][0]:
                        top_score = best[0][0]
                        tied = [x for x in best if x[0] == top_score]
                        score, g, b = rng.choice(tied)
                        return (g, b)
                    _, g, b = best[0]
                    return (g, b)

                def try_take(engine: str) -> bool:
                    chosen = choose_best(engine)
                    if chosen is None:
                        return False
                    g, b = chosen
                    for eng2, slots2 in b.items():
                        if eng2 == "debug":
                            continue
                        out[eng2].extend(slots2)
                        cap[eng2] -= len(slots2)
                    pos[g] += 1
                    used_groups.add(g)
                    return True

                for eng in engine_order:
                    while try_take(eng):
                        pass

                emit_bundle(out)

        # Best-performing heuristic so far: keep memory-heavy groups ahead and let lagging groups
        # consume compute to maximize overlap.
        self.instrs.extend(
            build_schedule(
                engine_order=("valu", "load", "store", "alu", "flow"),
                mem_ahead_compute_behind=True,
                rng_seed=None,
            )
        )

    def build_hash(self, val_hash_addr, tmp1, tmp2, round, i):
        slots = []

        for hi, (op1, val1, op2, op3, val3) in enumerate(HASH_STAGES):
            slots.append(("alu", (op1, tmp1, val_hash_addr, self.scratch_const(val1))))
            slots.append(("alu", (op3, tmp2, val_hash_addr, self.scratch_const(val3))))
            slots.append(("alu", (op2, val_hash_addr, tmp1, tmp2)))
            slots.append(("debug", ("compare", val_hash_addr, (round, i, "hash_stage", hi))))

        return slots

    def build_kernel(
        self, forest_height: int, n_nodes: int, batch_size: int, rounds: int
    ):
        """
        Like reference_kernel2 but building actual instructions.
        Scalar implementation using only scalar ALU and load/store.
        """
        # This submission kernel is specialized for the benchmark configuration.
        if not (forest_height == 10 and n_nodes == 2047 and batch_size == 256 and rounds == 16):
            raise NotImplementedError(
                "Optimized kernel is specialized for forest_height=10, n_nodes=2047, batch_size=256, rounds=16"
            )

        debug_pause = bool(os.environ.get("KERNEL_PAUSE"))

        # ---- Scalars ----
        s_forest_values_p = self.alloc_scratch("forest_values_p")
        s_inp_values_p = self.alloc_scratch("inp_values_p")
        io_addr_base = self.alloc_scratch("io_addr", batch_size // VLEN)

        const_inits: list[tuple] = []

        def alloc_const(name: str, value: int) -> int:
            addr = self.alloc_scratch(name)
            const_inits.append(("const", addr, value))
            return addr

        # Read pointers from the header: mem[4] and mem[6]
        s_const4 = alloc_const("const_4", 4)
        s_const6 = alloc_const("const_6", 6)

        # Constants used by the hash and idx update
        c0 = alloc_const("c0", 0x7ED55D16)
        c1 = alloc_const("c1", 0xC761C23C)
        c2 = alloc_const("c2", 0x165667B1)
        c3 = alloc_const("c3", 0xD3A2646C)
        c4 = alloc_const("c4", 0xFD7046C5)
        c5 = alloc_const("c5", 0xB55A4F09)
        s_one = alloc_const("one", 1)
        s_two = alloc_const("two", 2)
        s_three = alloc_const("three", 3)
        s_seven = alloc_const("seven", 7)
        # Depth bases (heap indices): base_d = 2^d - 1. We need depths 4..10 for gathers.
        s_base15 = alloc_const("base15", 15)
        s_base31 = alloc_const("base31", 31)
        s_base63 = alloc_const("base63", 63)
        s_base127 = alloc_const("base127", 127)
        s_base255 = alloc_const("base255", 255)
        s_base511 = alloc_const("base511", 511)
        s_base1023 = alloc_const("base1023", 1023)
        s_mask1 = alloc_const("mask1", 1)
        s_4097 = alloc_const("k4097", 4097)
        s_33 = alloc_const("k33", 33)
        s_9 = alloc_const("k9", 9)
        s_sh19 = alloc_const("sh19", 19)
        s_sh16 = alloc_const("sh16", 16)
        s_sh9 = alloc_const("sh9", 9)

        # Emit scalar constant loads packed (2 per cycle).
        for i in range(0, len(const_inits), 2):
            self.add_bundle({"load": const_inits[i : i + 2]})

        # Load pointers from the header.
        self.add_bundle(
            {
                "load": [
                    ("load", s_forest_values_p, s_const4),
                    ("load", s_inp_values_p, s_const6),
                ]
            }
        )

        # Precompute forest base addresses for depths 3..10 as scalars:
        # addr_d = forest_values_p + (2^d - 1)
        s_forest_d3 = self.alloc_scratch("forest_p_d3")
        s_forest_d4 = self.alloc_scratch("forest_p_d4")
        s_forest_d5 = self.alloc_scratch("forest_p_d5")
        s_forest_d6 = self.alloc_scratch("forest_p_d6")
        s_forest_d7 = self.alloc_scratch("forest_p_d7")
        s_forest_d8 = self.alloc_scratch("forest_p_d8")
        s_forest_d9 = self.alloc_scratch("forest_p_d9")
        s_forest_d10 = self.alloc_scratch("forest_p_d10")
        self.add_bundle({"alu": [("+", s_forest_d3, s_forest_values_p, s_seven)]})
        self.add_bundle({"alu": [("+", s_forest_d4, s_forest_values_p, s_base15)]})
        self.add_bundle({"alu": [("+", s_forest_d5, s_forest_values_p, s_base31)]})
        self.add_bundle({"alu": [("+", s_forest_d6, s_forest_values_p, s_base63)]})
        self.add_bundle({"alu": [("+", s_forest_d7, s_forest_values_p, s_base127)]})
        self.add_bundle({"alu": [("+", s_forest_d8, s_forest_values_p, s_base255)]})
        self.add_bundle({"alu": [("+", s_forest_d9, s_forest_values_p, s_base511)]})
        self.add_bundle({"alu": [("+", s_forest_d10, s_forest_values_p, s_base1023)]})

        # Broadcast vectors packed (6 per cycle).
        vec_inits: list[tuple[int, int]] = []

        def alloc_vbroadcast(name: str, scalar_addr: int) -> int:
            vaddr = self.alloc_scratch(name, VLEN)
            vec_inits.append((vaddr, scalar_addr))
            return vaddr

        v_one = alloc_vbroadcast("v_one", s_one)
        v_two = alloc_vbroadcast("v_two", s_two)
        v_three = alloc_vbroadcast("v_three", s_three)
        v_seven = alloc_vbroadcast("v_seven", s_seven)
        v_mask1 = alloc_vbroadcast("v_mask1", s_mask1)
        v_4097 = alloc_vbroadcast("v_4097", s_4097)
        v_33 = alloc_vbroadcast("v_33", s_33)
        v_9 = alloc_vbroadcast("v_9", s_9)
        v_c0 = alloc_vbroadcast("v_c0", c0)
        v_c1 = alloc_vbroadcast("v_c1", c1)
        v_c2 = alloc_vbroadcast("v_c2", c2)
        v_c3 = alloc_vbroadcast("v_c3", c3)
        v_c4 = alloc_vbroadcast("v_c4", c4)
        v_c5 = alloc_vbroadcast("v_c5", c5)
        v_sh19 = alloc_vbroadcast("v_sh19", s_sh19)
        v_sh16 = alloc_vbroadcast("v_sh16", s_sh16)
        v_sh9 = alloc_vbroadcast("v_sh9", s_sh9)
        v_forest_d3 = alloc_vbroadcast("v_forest_p_d3", s_forest_d3)
        v_forest_d4 = alloc_vbroadcast("v_forest_p_d4", s_forest_d4)
        v_forest_d5 = alloc_vbroadcast("v_forest_p_d5", s_forest_d5)
        v_forest_d6 = alloc_vbroadcast("v_forest_p_d6", s_forest_d6)
        v_forest_d7 = alloc_vbroadcast("v_forest_p_d7", s_forest_d7)
        v_forest_d8 = alloc_vbroadcast("v_forest_p_d8", s_forest_d8)
        v_forest_d9 = alloc_vbroadcast("v_forest_p_d9", s_forest_d9)
        v_forest_d10 = alloc_vbroadcast("v_forest_p_d10", s_forest_d10)

        for i in range(0, len(vec_inits), SLOT_LIMITS["valu"]):
            slots = [("vbroadcast", vaddr, saddr) for (vaddr, saddr) in vec_inits[i : i + SLOT_LIMITS["valu"]]]
            self.add_bundle({"valu": slots})

        # ---- Preload top-of-tree node values and build selector coefficients ----
        # Nodes 0..6 are used in rounds (depths 0..2) twice; replacing gathers there
        # cuts load pressure substantially while keeping valu-slot overhead low.
        top0 = self.alloc_scratch("top_nodes_0_7", VLEN)

        # vload nodes 0..7
        self.add_bundle({"load": [("vload", top0, s_forest_values_p)]})

        # Scalar coefficient scratch (computed from loaded node scalars).
        s_d1_1 = self.alloc_scratch("d1_1")  # node2 - node1
        s_d2_1 = self.alloc_scratch("d2_1")  # node4 - node3
        s_d2_2 = self.alloc_scratch("d2_2")  # node5 - node3
        s_d2_3 = self.alloc_scratch("d2_3")  # node6 - node5 - node4 + node3
        s_tmp = self.alloc_scratch("coef_tmp")

        n0 = top0 + 0
        n1 = top0 + 1
        n2 = top0 + 2
        n3 = top0 + 3
        n4 = top0 + 4
        n5 = top0 + 5
        n6 = top0 + 6
        # n7 = top0 + 7 (unused)

        # Depth-1: d1_1 = n2 - n1
        self.add_bundle({"alu": [("-", s_d1_1, n2, n1)]})
        # Depth-2:
        self.add_bundle({"alu": [("-", s_d2_1, n4, n3), ("-", s_d2_2, n5, n3)]})
        # d2_3 = n6 - n5 - n4 + n3
        self.add_bundle({"alu": [("-", s_tmp, n6, n5)]})
        self.add_bundle({"alu": [("-", s_tmp, s_tmp, n4)]})
        self.add_bundle({"alu": [("+", s_d2_3, s_tmp, n3)]})

        # Broadcast node0 and depth<=2 coefficient vectors.
        v_node0 = self.alloc_scratch("v_node0", VLEN)
        v_d1_0 = self.alloc_scratch("v_d1_0", VLEN)
        v_d1_1 = self.alloc_scratch("v_d1_1", VLEN)
        v_d2_0 = self.alloc_scratch("v_d2_0", VLEN)
        v_d2_1 = self.alloc_scratch("v_d2_1", VLEN)
        v_d2_2 = self.alloc_scratch("v_d2_2", VLEN)
        v_d2_3 = self.alloc_scratch("v_d2_3", VLEN)

        vb = []
        vb.append(("vbroadcast", v_node0, n0))
        vb.append(("vbroadcast", v_d1_0, n1))
        vb.append(("vbroadcast", v_d1_1, s_d1_1))
        vb.append(("vbroadcast", v_d2_0, n3))
        vb.append(("vbroadcast", v_d2_1, s_d2_1))
        vb.append(("vbroadcast", v_d2_2, s_d2_2))
        vb.append(("vbroadcast", v_d2_3, s_d2_3))
        for i in range(0, len(vb), SLOT_LIMITS["valu"]):
            self.add_bundle({"valu": vb[i : i + SLOT_LIMITS["valu"]]})

        # ---- Per-lane state in scratch (SoA, 256 lanes) ----
        # Track the per-depth path code instead of the full heap index:
        # idx(depth) = (2^depth - 1) + path, and path updates as path = path*2 + (val & 1).
        path_base = self.alloc_scratch("path", batch_size)
        val_base = self.alloc_scratch("val", batch_size)
        t1_base = self.alloc_scratch("t1", batch_size)
        t2_base = self.alloc_scratch("t2", batch_size)

        # Optional pauses for local debugging harness.
        if debug_pause:
            self.add_bundle({"flow": [("pause",)]})

        # Load input values into scratch once (indices are all zero initially).
        self.add_bundle({"flow": [("add_imm", io_addr_base + 0, s_inp_values_p, 0)]})
        for g in range(batch_size // VLEN):
            bundle: dict[str, list[tuple]] = {
                "load": [("vload", val_base + g * VLEN, io_addr_base + g)]
            }
            if g + 1 < batch_size // VLEN:
                bundle["flow"] = [("add_imm", io_addr_base + g + 1, io_addr_base + g, VLEN)]
            self.add_bundle(bundle)
        # path initialized to 0 by default scratch contents.

        def append_round_ops(
            ops: list[dict[str, list[tuple]]],
            *,
            path: int,
            val: int,
            t1: int,
            t2: int,
            r: int,
            do_update: bool,
            do_wrap: bool,
        ):
            # Replace gathers for depths 0..2 (rounds 0..2 and 11..13) with a small constant-time selector.
            if r in (0, 11):
                # idx == 0 for all lanes at depth 0
                ops.append({"valu": [("^", val, val, v_node0)]})
            elif r in (1, 12):
                # depth-1: idx in {1,2} -> path in {0,1}
                ops.append({"valu": [("multiply_add", t1, v_d1_1, path, v_d1_0)]})
                ops.append({"valu": [("^", val, val, t1)]})
            elif r in (2, 13):
                # depth-2: idx in [3..6] -> path j in [0..3]
                # t2 = b0 = (j & 1)
                ops.append({"valu": [("&", t2, path, v_mask1)]})
                # t1 = node3 + b0*(node4-node3)
                ops.append({"valu": [("multiply_add", t1, v_d2_1, t2, v_d2_0)]})
                # t2 = b1 = (j >> 1)  (j is 0..3 so already 0/1)
                ops.append({"valu": [(">>", t2, path, v_one)]})
                ops.append({"valu": [("multiply_add", t1, v_d2_2, t2, t1)]})
                # t2 = b01 = (j == 3)
                ops.append({"valu": [("==", t2, path, v_three)]})
                ops.append({"valu": [("multiply_add", t1, v_d2_3, t2, t1)]})
                ops.append({"valu": [("^", val, val, t1)]})
            else:
                # Gather node values from the current depth's contiguous block:
                # addr = forest_values_p + base(depth) + path
                depth = r if r <= 10 else (r - 11)
                v_forest_depth = (
                    v_forest_d3
                    if depth == 3
                    else v_forest_d4
                    if depth == 4
                    else v_forest_d5
                    if depth == 5
                    else v_forest_d6
                    if depth == 6
                    else v_forest_d7
                    if depth == 7
                    else v_forest_d8
                    if depth == 8
                    else v_forest_d9
                    if depth == 9
                    else v_forest_d10
                )
                ops.append({"valu": [("+", t1, path, v_forest_depth)]})
                # Gather node values into t1 (overwriting addresses) with 2 loads/cycle
                ops.append({"load": [("load_offset", t1, t1, 0), ("load_offset", t1, t1, 1)]})
                ops.append({"load": [("load_offset", t1, t1, 2), ("load_offset", t1, t1, 3)]})
                ops.append({"load": [("load_offset", t1, t1, 4), ("load_offset", t1, t1, 5)]})
                ops.append({"load": [("load_offset", t1, t1, 6), ("load_offset", t1, t1, 7)]})
                # val ^= node_val
                ops.append({"valu": [("^", val, val, t1)]})

            # Hash stages (fused where possible)
            # 0: a = a*4097 + c0
            ops.append({"valu": [("multiply_add", val, val, v_4097, v_c0)]})
            # 1: a = (a ^ c1) ^ (a >> 19)
            ops.append({"valu": [(">>", t1, val, v_sh19), ("^", t2, val, v_c1)]})
            ops.append({"valu": [("^", val, t1, t2)]})
            # 2: a = a*33 + c2
            ops.append({"valu": [("multiply_add", val, val, v_33, v_c2)]})
            # 3: a = (a + c3) ^ (a << 9)
            ops.append({"valu": [("<<", t1, val, v_sh9), ("+", t2, val, v_c3)]})
            ops.append({"valu": [("^", val, t1, t2)]})
            # 4: a = a*9 + c4
            ops.append({"valu": [("multiply_add", val, val, v_9, v_c4)]})
            # 5: a = (a ^ c5) ^ (a >> 16)
            ops.append({"valu": [(">>", t1, val, v_sh16), ("^", t2, val, v_c5)]})
            ops.append({"valu": [("^", val, t1, t2)]})

            if do_update:
                if do_wrap:
                    # After round 10, the next idx wraps to 0 unconditionally for this benchmark.
                    ops.append({"valu": [("^", path, path, path)]})
                elif r in (0, 11):
                    # path starts at 0, so next_path = (val & 1)
                    ops.append({"valu": [("&", path, val, v_mask1)]})
                else:
                    # path = path*2 + (val & 1)
                    ops.append({"valu": [("&", t1, val, v_mask1)]})
                    ops.append({"valu": [("multiply_add", path, path, v_two, t1)]})

        # Build a single global schedule across all groups to avoid per-round pipeline bubbles.
        groups_ops: list[list[dict[str, list[tuple]]]] = []
        for g in range(batch_size // VLEN):
            path = path_base + g * VLEN
            val = val_base + g * VLEN
            t1 = t1_base + g * VLEN
            t2 = t2_base + g * VLEN

            ops: list[dict[str, list[tuple]]] = []

            for r in range(rounds):
                append_round_ops(
                    ops,
                    path=path,
                    val=val,
                    t1=t1,
                    t2=t2,
                    r=r,
                    do_update=(r != rounds - 1),
                    do_wrap=(r == 10),
                )
            groups_ops.append(ops)

        self._schedule_round(groups_ops)

        # Store values back to memory.
        self.add_bundle({"flow": [("add_imm", io_addr_base + 0, s_inp_values_p, 0)]})
        for g in range(batch_size // VLEN):
            bundle: dict[str, list[tuple]] = {
                "store": [("vstore", io_addr_base + g, val_base + g * VLEN)]
            }
            if g + 1 < batch_size // VLEN:
                bundle["flow"] = [("add_imm", io_addr_base + g + 1, io_addr_base + g, VLEN)]
            self.add_bundle(bundle)

        if debug_pause:
            self.add_bundle({"flow": [("pause",)]})

BASELINE = 147734

def do_kernel_test(
    forest_height: int,
    rounds: int,
    batch_size: int,
    seed: int = 123,
    trace: bool = False,
    prints: bool = False,
):
    print(f"{forest_height=}, {rounds=}, {batch_size=}")
    random.seed(seed)
    forest = Tree.generate(forest_height)
    inp = Input.generate(forest, batch_size, rounds)
    mem = build_mem_image(forest, inp)

    kb = KernelBuilder()
    kb.build_kernel(forest.height, len(forest.values), len(inp.indices), rounds)
    # print(kb.instrs)

    value_trace = {}
    machine = Machine(
        mem,
        kb.instrs,
        kb.debug_info(),
        n_cores=N_CORES,
        value_trace=value_trace,
        trace=trace,
    )
    machine.prints = prints
    for i, ref_mem in enumerate(reference_kernel2(mem, value_trace)):
        machine.run()
        inp_values_p = ref_mem[6]
        if prints:
            print(machine.mem[inp_values_p : inp_values_p + len(inp.values)])
            print(ref_mem[inp_values_p : inp_values_p + len(inp.values)])
        assert (
            machine.mem[inp_values_p : inp_values_p + len(inp.values)]
            == ref_mem[inp_values_p : inp_values_p + len(inp.values)]
        ), f"Incorrect result on round {i}"
        inp_indices_p = ref_mem[5]
        if prints:
            print(machine.mem[inp_indices_p : inp_indices_p + len(inp.indices)])
            print(ref_mem[inp_indices_p : inp_indices_p + len(inp.indices)])
        # Updating these in memory isn't required, but you can enable this check for debugging
        # assert machine.mem[inp_indices_p:inp_indices_p+len(inp.indices)] == ref_mem[inp_indices_p:inp_indices_p+len(inp.indices)]

    print("CYCLES: ", machine.cycle)
    print("Speedup over baseline: ", BASELINE / machine.cycle)
    return machine.cycle


class Tests(unittest.TestCase):
    def test_ref_kernels(self):
        """
        Test the reference kernels against each other
        """
        random.seed(123)
        for i in range(10):
            f = Tree.generate(4)
            inp = Input.generate(f, 10, 6)
            mem = build_mem_image(f, inp)
            reference_kernel(f, inp)
            for _ in reference_kernel2(mem, {}):
                pass
            assert inp.indices == mem[mem[5] : mem[5] + len(inp.indices)]
            assert inp.values == mem[mem[6] : mem[6] + len(inp.values)]

    def test_kernel_trace(self):
        # Full-scale example for performance testing
        do_kernel_test(10, 16, 256, trace=True, prints=False)

    # Passing this test is not required for submission, see submission_tests.py for the actual correctness test
    # You can uncomment this if you think it might help you debug
    # def test_kernel_correctness(self):
    #     for batch in range(1, 3):
    #         for forest_height in range(3):
    #             do_kernel_test(
    #                 forest_height + 2, forest_height + 4, batch * 16 * VLEN * N_CORES
    #             )

    def test_kernel_cycles(self):
        do_kernel_test(10, 16, 256)


# To run all the tests:
#    python perf_takehome.py
# To run a specific test:
#    python perf_takehome.py Tests.test_kernel_cycles
# To view a hot-reloading trace of all the instructions:  **Recommended debug loop**
# NOTE: The trace hot-reloading only works in Chrome. In the worst case if things aren't working, drag trace.json onto https://ui.perfetto.dev/
#    python perf_takehome.py Tests.test_kernel_trace
# Then run `python watch_trace.py` in another tab, it'll open a browser tab, then click "Open Perfetto"
# You can then keep that open and re-run the test to see a new trace.

# To run the proper checks to see which thresholds you pass:
#    python tests/submission_tests.py

if __name__ == "__main__":
    unittest.main()
